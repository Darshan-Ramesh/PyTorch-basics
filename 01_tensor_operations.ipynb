{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy of 01-tensor-operations.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l-dgS0DJnzK",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch basics on tensors!\n",
        "\n",
        "###  \n",
        "\n",
        "Pytorch is an amazing computational library which brings pythonic style of coding brings complex deep learning concepts right under your fingers!\n",
        "Let's discuss more about the fundamental concepts and frequently used basic tensor functions that you would see very often in any github code using PyTorch!\n",
        "\n",
        "- torch.Tensor vs torch.tensor vs torch.as_tensor()\n",
        "- torch.view()\n",
        "- tensor.permute()\n",
        "- tensor.device()\n",
        "- tensor.reshape_()\n",
        "- tensor.resize()\n",
        "- torch.squeeze()\n",
        "- torch.unsqueeze()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkKCqdIxJnzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import torch and other required modules\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKpFU9rDJnzU",
        "colab_type": "text"
      },
      "source": [
        "## Function 1 - torch.tensor() vs torch.Tensor() vs torch.as_tensor()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqx41jWCJnzV",
        "colab_type": "code",
        "outputId": "89c5c652-7049-4096-e904-ea148b376729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Example 1 \n",
        "data = [1,2,3,4]\n",
        "t1 = torch.Tensor(data)\n",
        "print(t1)\n",
        "print(t1.dtype)           "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3., 4.])\n",
            "torch.float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmRwntdB9p9t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "271082b7-75ae-40bc-839d-270316ca396d"
      },
      "source": [
        "t2= torch.tensor(data)\n",
        "print(t2)\n",
        "print(t2.dtype)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3, 4])\n",
            "torch.int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E-HDAwTA2kx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f1e27f93-af22-47fd-f168-604790550f14"
      },
      "source": [
        "import numpy as np\n",
        "data2= np.array([1,2,3])\n",
        "print(f'data type of data2 is - {type(data2)}')\n",
        "\n",
        "#converting a numpy array to a tensor \n",
        "t3 = torch.as_tensor(data2)\n",
        "print(f'data type of tensor t3 is - {t3.dtype}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data type of data2 is - <class 'numpy.ndarray'>\n",
            "data type of tensor t3 is - torch.int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ3o7c-PJnze",
        "colab_type": "text"
      },
      "source": [
        "As you see it from the above example, the tensor 't1= torch.Tensor()' will convert the data into the default datatype-> float32, while the tensor 't2' with 'torch.tensor()' will look at the data and preserves its datatype! Also, 'tensor_astensor()' avoids creating the copy of varibles. This is because arguments to this function is passed by reference (pointers) rather than passing the values. This is very interesting implementation in software designing, known as 'The Factory function' design pattern. It is a design pattern used to define a runtime interface for creating an object. Itâ€™s called a factory because it creates various types of objects without necessarily knowing what kind of object it creates or how to create it.[1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YfO-DUAVQ08",
        "colab_type": "text"
      },
      "source": [
        "## Function - 2 - torch.view()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWx_QHZ9Jnzf",
        "colab_type": "code",
        "outputId": "1f45438a-f630-4978-9d40-4c68ebcfc37f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Example 2\n",
        "t4 = torch.rand(4, 4) #generate a matrix with rows=4 and cols=4 with randon numbers between 0 and 1 taken from uniform distribution\n",
        "print(f't4 - {t4}\\n')\n",
        "\n",
        "#reshaping the tensors with new shape. This does not create a copy of t4 but works on t4 directly by passing values as reference than values.\n",
        "new_t4 = t4.view(2,8)\n",
        "print(f'new_tf - {new_t4}\\n')\n",
        "\n",
        "\n",
        "#so modifying first value in 'new_t4' must reflect in 't4'\n",
        "new_t4[0][0] = 0.5\n",
        "print(f't4 - {t4}')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t4 - tensor([[0.8460, 0.8770, 0.3523, 0.4718],\n",
            "        [0.2623, 0.1998, 0.7822, 0.9587],\n",
            "        [0.5182, 0.7398, 0.7883, 0.0823],\n",
            "        [0.0253, 0.8231, 0.5043, 0.0912]])\n",
            "\n",
            "new_tf - tensor([[0.8460, 0.8770, 0.3523, 0.4718, 0.2623, 0.1998, 0.7822, 0.9587],\n",
            "        [0.5182, 0.7398, 0.7883, 0.0823, 0.0253, 0.8231, 0.5043, 0.0912]])\n",
            "\n",
            "t4 - tensor([[0.5000, 0.8770, 0.3523, 0.4718],\n",
            "        [0.2623, 0.1998, 0.7822, 0.9587],\n",
            "        [0.5182, 0.7398, 0.7883, 0.0823],\n",
            "        [0.0253, 0.8231, 0.5043, 0.0912]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8VO85Z6Jnzo",
        "colab_type": "text"
      },
      "source": [
        "'View' is a very powerful implementation which is often used in computer vision to reshape the tensors! These tensors shares the same underlying data with its base tensor, meaning any modification to the new tensor made will be reflected in its parent/base tensor. This is possible due to its pass by reference implementation rather than pass by value implementation which avoids copy of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyFBjxMWJnzp",
        "colab_type": "code",
        "outputId": "31e9112b-84bb-4b4f-8e21-603aa5930156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "# Example 3 - breaking \n",
        "#So from the above example, the dimenality must be preserved, else it throws an error. For eg\n",
        "t5 = new_t4.view(2,2)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6c264c7921a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#So from the above example, the dimenality must be preserved, else it throws an error. For eg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_t4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 2]' is invalid for input of size 16"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46IGEIKVJnzz",
        "colab_type": "text"
      },
      "source": [
        "It clearly mentions that you cannot reshape a 4x4 martix into a 2x2 matrix, which can be done using slicing but not reshpaing! The correct implmentation would be.. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG9ODCB0Yqy0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "3bdb3d5d-b97d-4679-e86e-264074b02243"
      },
      "source": [
        "t5 = new_t4.view(8,-1)   # in scenarios where you don't know the column/row dimensions, you can simply pass -1 to the col/row argument!\n",
        "# t5 = new_t4.view(1,16)\n",
        "# t5 = new_t4.view(16,1)\n",
        "print(f't5 - {t5}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t5 - tensor([[0.5000, 0.8770],\n",
            "        [0.3523, 0.4718],\n",
            "        [0.2623, 0.1998],\n",
            "        [0.7822, 0.9587],\n",
            "        [0.5182, 0.7398],\n",
            "        [0.7883, 0.0823],\n",
            "        [0.0253, 0.8231],\n",
            "        [0.5043, 0.0912]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRI9nFTPJnz3",
        "colab_type": "text"
      },
      "source": [
        "## Function 3 - torch.permute()\n",
        "\n",
        "This is related to the 'transpose' function in the python and to View in PyTorch!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR3F8d4GJnz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "7555d85a-cf76-4e35-cd3d-76e28c267540"
      },
      "source": [
        "# Example 1\n",
        "data3 = torch.rand(3,2,5) #this says tha the tensor must have 2 rows and 5 columns but 2 batches(batch_first) of data\n",
        "print(f'data3_shape - {data3.shape}')\n",
        "print(f'data3 is - {data3}\\n')\n",
        "\n",
        "\n",
        "#now if we want to transpose this into a  batch_last option without any creating data copy, then\n",
        "data4 = data3.permute(1,2,0)\n",
        "print(f'data4_shape - {data4.shape}')\n",
        "print(f'data4 - {data4}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data3_shape - torch.Size([3, 2, 5])\n",
            "data3 is - tensor([[[0.5612, 0.7003, 0.6685, 0.7845, 0.1161],\n",
            "         [0.9358, 0.9018, 0.7310, 0.2631, 0.6020]],\n",
            "\n",
            "        [[0.6280, 0.0307, 0.8031, 0.6961, 0.5546],\n",
            "         [0.1915, 0.7227, 0.5936, 0.4988, 0.1031]],\n",
            "\n",
            "        [[0.6714, 0.0198, 0.7395, 0.9165, 0.6125],\n",
            "         [0.9296, 0.5828, 0.4076, 0.8963, 0.6584]]])\n",
            "\n",
            "data4_shape - torch.Size([2, 5, 3])\n",
            "data4 - tensor([[[0.5612, 0.6280, 0.6714],\n",
            "         [0.7003, 0.0307, 0.0198],\n",
            "         [0.6685, 0.8031, 0.7395],\n",
            "         [0.7845, 0.6961, 0.9165],\n",
            "         [0.1161, 0.5546, 0.6125]],\n",
            "\n",
            "        [[0.9358, 0.1915, 0.9296],\n",
            "         [0.9018, 0.7227, 0.5828],\n",
            "         [0.7310, 0.5936, 0.4076],\n",
            "         [0.2631, 0.4988, 0.8963],\n",
            "         [0.6020, 0.1031, 0.6584]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1DTaqNlJn0E",
        "colab_type": "text"
      },
      "source": [
        "This is also very oftenly used to swap the order of the tensors without creating a copy of the existing data similar to 'View' operation. Normally we come across this operaton in deep learning for vision where we would need to swap the dimensions of images as either (batch_size,W,H,channel) with (W,H,channel,batch_size) or vice-verse. A very handly function to know about.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYh_iIr_Jn0R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "6f3df2d3-a742-46ff-e4fc-5fafdfb74743"
      },
      "source": [
        "# Example2\n",
        "# NOTE: The permute method takes the 'indices' of the shape of the existing tensor and not some random numbers.\n",
        "# If you try to access the values beyond the 'indices' 'you get dimension out of range error'\n",
        "\n",
        "print(f'shape of data4 is - {data4.shape}\\n')\n",
        "for i in range(0,3):\n",
        "  print(f'index {i} - {data4.shape[i]} ')\n",
        "print(f'New shape of data4 after permute is - {data4.permute(0,2,3)}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of data4 is - torch.Size([2, 5, 3])\n",
            "\n",
            "index 0 - 2 \n",
            "index 1 - 5 \n",
            "index 2 - 3 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1c7a0c2547cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'index {i} - {data4.shape[i]} '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'New shape of data4 after permute is - {data4.permute(0,2,3)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBdEufOjJn0Y",
        "colab_type": "text"
      },
      "source": [
        "It can be seen from the above exmaple that the 'permute()' function takes the indices of the shape of given tensor and swaps the rows, cols and other dimensions inline! If any value other than the indices are provided it throws an error, one such example which I often encounter is the dimension out of range error!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXGzBDLxJn0b",
        "colab_type": "text"
      },
      "source": [
        "## Function 4 - torch.device\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV-0iVOCJn0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "a2bd6273-c8d9-416b-d8c9-3b016a41b421"
      },
      "source": [
        "#checks if the cuda is available on your device\n",
        "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device - {dev}')\n",
        "t6 = torch.rand(4,4,dtype=torch.float64,device=dev)\n",
        "print(t6)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device - cuda\n",
            "tensor([[0.4748, 0.5703, 0.5160, 0.7933],\n",
            "        [0.3456, 0.4909, 0.6322, 0.7953],\n",
            "        [0.3250, 0.1766, 0.7195, 0.1941],\n",
            "        [0.1241, 0.1314, 0.7593, 0.5525]], device='cuda:0',\n",
            "       dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh40Umgem6Ut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2d14cf7e-8c8c-4ad6-f67c-76b640dee899"
      },
      "source": [
        "#you can still enforce a tensor to be on the cpu by doing the following \n",
        "t7 = torch.rand(4,4,dtype=torch.float64,device='cpu')\n",
        "print(f't7 is on - {t7.device}')\n",
        "if torch.cuda.is_available():\n",
        "  print('Hey GPU is available, copying the tensor into GPUs memory')\n",
        "  t7 = t7.cuda()\n",
        "  print(f'Now t7 is on - {t7.device}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t7 is on - cpu\n",
            "Hey GPU is available, copying the tensor into GPUs memory\n",
            "Now t7 is on - cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEyGRmj7Jn0k",
        "colab_type": "text"
      },
      "source": [
        "This is an amazing functionality where you can check the memory allocation of \n",
        "the tensors. Is the tensor on the CPU memory or GPU memory. By default tensors are on cpu unless specifically mentioned or copied from cpu to gpu memory.\n",
        "This attribute is very frequently used doing training the deep learning models while training. If a device is GPU enabled, but for some reason you need to train models using CPU, just by sending a 'string' "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGis-Tv7Jn0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "a033643f-3d2e-4a19-91d0-2015ebf55e7f"
      },
      "source": [
        "#the strings that must be given to the 'device' option is from a known list, any other string given will throw an error.\n",
        "t8 = torch.randn(4,4,device='gpu')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f5e1981af7a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#the strings that must be given to the 'device' option is from a known list, any other string given will throw an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, mkldnn, opengl, opencl, ideep, hip, msnpu device type at start of device string: gpu"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2jz_83FtIZL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "44493acf-a39d-4e7f-a901-faf293454e0e"
      },
      "source": [
        "#optionally you can also do this\n",
        "cuda_device = torch.device('cuda:0')\n",
        "t8= torch.rand(4,4,device=cuda_device)\n",
        "t8"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6250, 0.1002, 0.5522, 0.5335],\n",
              "        [0.0229, 0.8209, 0.0298, 0.7981],\n",
              "        [0.7002, 0.2035, 0.2941, 0.9794],\n",
              "        [0.8305, 0.1309, 0.4619, 0.2343]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Pl9njnJn04",
        "colab_type": "text"
      },
      "source": [
        "## Function 5 - tensor.resize_()\n",
        "\n",
        "tensor.resize_() is an interesting method which splits the parts of a tensor with certain shape and copies the data from it to form a new shape. So, the new tensor being created can have \n",
        "\n",
        "\n",
        "1.   A shape greater than the shape of the tensor being converted, and\n",
        "2.   A shape smaller than the shape of the tensor being converted\n",
        "\n",
        "In the former case, the underlying data/tensor's shape is first converted and then a new memory is filled with the resized data. But in the latter case, the underlying data/tensors are not changed but existing data is copied into a new memory!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D79xIZPBJn05",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "0c88e688-34c8-44d5-871d-8479905ec98f"
      },
      "source": [
        "data5 = torch.rand(4,4)\n",
        "print(f'data5 - {data5}, shape - {data5.shape}\\n')\n",
        "\n",
        "#case: new tensor is being resized to a lesser shape\n",
        "resized_data5_small = data5.resize_(3,2)\n",
        "print(f'resized_data5_small - {resized_data5_small}, shape - {resized_data5_small.shape}\\n')\n",
        "\n",
        "#case: new tensor is being resized to a lesser shape\n",
        "resized_data5_large = data5.resize_(5,5)\n",
        "print(f'resized_data5_small - {resized_data5_large}, shape - {resized_data5_large.shape}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data5 - tensor([[0.4278, 0.9469, 0.3868, 0.5673],\n",
            "        [0.9997, 0.4777, 0.6988, 0.6179],\n",
            "        [0.8054, 0.5041, 0.5840, 0.4235],\n",
            "        [0.0591, 0.8856, 0.9506, 0.5793]]), shape - torch.Size([4, 4])\n",
            "\n",
            "resized_data5_small - tensor([[0.4278, 0.9469],\n",
            "        [0.3868, 0.5673],\n",
            "        [0.9997, 0.4777]]), shape - torch.Size([3, 2])\n",
            "\n",
            "resized_data5_small - tensor([[ 4.2782e-01,  9.4690e-01,  3.8676e-01,  5.6727e-01,  9.9970e-01],\n",
            "        [ 4.7770e-01,  6.9882e-01,  6.1789e-01,  8.0540e-01,  5.0414e-01],\n",
            "        [ 5.8399e-01,  4.2347e-01,  5.9140e-02,  8.8559e-01,  9.5057e-01],\n",
            "        [ 5.7930e-01,  1.0842e-19,  1.8263e+00,  0.0000e+00,  1.7510e+00],\n",
            "        [-1.0842e-19,  1.7710e+00, -0.0000e+00,  1.7117e+00,  0.0000e+00]]), shape - torch.Size([5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE-0fk6mMSw6",
        "colab_type": "text"
      },
      "source": [
        "## Function 6 - tensor.reshape() \n",
        "- This function is very similar to the 'View' function which we saw earlier. But these can operate even on the non-contiguity tensors. \n",
        "\n",
        "\n",
        "\n",
        "1.   Contiguity data is when the data tensors or arrays are stored in the memory\n",
        "row  wise. Please follow [this](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch) and [this](https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092) for the excellent explanations about it. So, its easy and faster to obtain the addresses of the contiguous rows if the data are stored are in contiguity fashion. 'View' function that we saw earlier imposes some contiguity constraints on the shapes of the two tensors and might break sometimes but reshape function can work with non-contiguity data also!\n",
        "2.   In simpler terms, 'view' works only if tensor.is_contiguous()==True and 'reshape' works with any kind of tensors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asCVWTcRMRyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "9ee846f2-7e46-45ef-b32b-4f170ec8a2af"
      },
      "source": [
        "data6 = torch.rand(3,4, dtype=torch.float32)\n",
        "print(f'data6 before reshaping - data6-{data6} - shape- {data6.shape}\\n')\n",
        "\n",
        "reshape_data6 = data6.reshape(2,-1)\n",
        "print(f'data6 before reshaping - data6-{reshape_data6} - shape- {reshape_data6.shape}\\n')\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data6 before reshaping - data6-tensor([[8.3931e-01, 6.7791e-01, 2.0965e-02, 2.6047e-01],\n",
            "        [3.2455e-04, 9.9850e-01, 6.6238e-01, 3.2699e-01],\n",
            "        [9.8790e-01, 2.0600e-01, 3.1494e-01, 4.5801e-01]]) - shape- torch.Size([3, 4])\n",
            "\n",
            "data6 before reshaping - data6-tensor([[8.3931e-01, 6.7791e-01, 2.0965e-02, 2.6047e-01, 3.2455e-04, 9.9850e-01],\n",
            "        [6.6238e-01, 3.2699e-01, 9.8790e-01, 2.0600e-01, 3.1494e-01, 4.5801e-01]]) - shape- torch.Size([2, 6])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aSXwbj0SDmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "0d38445e-3acc-4303-828b-45cc3eed8406"
      },
      "source": [
        "#breaking\n",
        "transposed_data6 = data6.t()\n",
        "print(f'transposed_data6 - {transposed_data6} and shape - {transposed_data6.shape}')\n",
        "transposed_data6.view(12,-1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transposed_data6 - tensor([[8.3931e-01, 3.2455e-04, 9.8790e-01],\n",
            "        [6.7791e-01, 9.9850e-01, 2.0600e-01],\n",
            "        [2.0965e-02, 6.6238e-01, 3.1494e-01],\n",
            "        [2.6047e-01, 3.2699e-01, 4.5801e-01]]) and shape - torch.Size([4, 3])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-2b2d7fb7f37f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtransposed_data6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'transposed_data6 - {transposed_data6} and shape - {transposed_data6.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtransposed_data6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49i7T0toTH7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3ff1b588-3710-4071-9605-35ba2bea52eb"
      },
      "source": [
        "transposed_data6.reshape(12)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([8.3931e-01, 3.2455e-04, 9.8790e-01, 6.7791e-01, 9.9850e-01, 2.0600e-01,\n",
              "        2.0965e-02, 6.6238e-01, 3.1494e-01, 2.6047e-01, 3.2699e-01, 4.5801e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVtRVaMNTS7f",
        "colab_type": "text"
      },
      "source": [
        "# Function 7 - torch.squeeze()\n",
        "\n",
        "Another interesting method is squeeze methof, which returns a tensor with all the dimensions of input of size 1 removed. Also, the returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiRfhAUfVKl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "34082001-7e76-4bbf-b523-b21a19901ba8"
      },
      "source": [
        "t = torch.ones(2,1,2,1) \n",
        "print(f't - {t}, shape - {t.shape}\\n')\n",
        "\n",
        "r = torch.squeeze(t)     # Size 2x2\n",
        "print(f'r - {r}, shape- {r.shape}')\n",
        "\n",
        "r = torch.squeeze(t, 1) \n",
        "print(f'r - {r}, shape- {r.shape}')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t - tensor([[[[1.],\n",
            "          [1.]]],\n",
            "\n",
            "\n",
            "        [[[1.],\n",
            "          [1.]]]]), shape - torch.Size([2, 1, 2, 1])\n",
            "\n",
            "r - tensor([[1., 1.],\n",
            "        [1., 1.]]), shape- torch.Size([2, 2])\n",
            "r - tensor([[[1.],\n",
            "         [1.]],\n",
            "\n",
            "        [[1.],\n",
            "         [1.]]]), shape- torch.Size([2, 2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_44ROdlWKuS",
        "colab_type": "text"
      },
      "source": [
        "# Function 8 - torch.unsqueeze()\n",
        "\n",
        "Just like squeeze, this unsqueeze a tensor and returns a new tensor with a dimension of size one inserted at the specified position and the returned tensor shares the same underlying data with this tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0GbRGBOWJ40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "26af9c9b-3e96-470c-c84c-41c8520f7841"
      },
      "source": [
        "x = torch.ones(3,4)\n",
        "print(f'x- {x} and shape- {x.shape}\\n')\n",
        "\n",
        "print(f'unsqueeze - {torch.unsqueeze(x,1)}, shape -{torch.unsqueeze(x,1).shape}\\n') #unsquezze at the index=1 of tensor x\n",
        "print(f'unsqueeze - {torch.unsqueeze(x,0)}, shape -{torch.unsqueeze(x,0).shape}\\n') #unsquezze at the index=0 of tensor x\n",
        "print(f'unsqueeze - {torch.unsqueeze(x,-1)}, shape -{torch.unsqueeze(x,-1).shape}') #unsquezze at the index=2/index= -1 of tensor x\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x- tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]]) and shape- torch.Size([3, 4])\n",
            "\n",
            "unsqueeze - tensor([[[1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.]]]), shape -torch.Size([3, 1, 4])\n",
            "\n",
            "unsqueeze - tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]]), shape -torch.Size([1, 3, 4])\n",
            "\n",
            "unsqueeze - tensor([[[1.],\n",
            "         [1.],\n",
            "         [1.],\n",
            "         [1.]],\n",
            "\n",
            "        [[1.],\n",
            "         [1.],\n",
            "         [1.],\n",
            "         [1.]],\n",
            "\n",
            "        [[1.],\n",
            "         [1.],\n",
            "         [1.],\n",
            "         [1.]]]), shape -torch.Size([3, 4, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5_EBDVSJn1w",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "We looked into the basic operations of tensors which are very imporant in while training or developing a vision based deep learning model! We will see in further posts on how these concepts will be used to develop regression models/ classification models using PyTorch's advacned methods and concepts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrXOSKfFJn1y",
        "colab_type": "text"
      },
      "source": [
        "## Reference Links\n",
        "Provide links to your references and other interesting articles about tensors\n",
        "* Official documentation for `torch.Tensor`: https://pytorch.org/docs/stable/tensors.html\n",
        "* [1] https://medium.com/secure-and-private-ai-writing-challenge/introduction-to-tensors-2-using-pytorch-2b6270a838f\n",
        "* https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\n",
        "* https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyqOpZgKJn10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install jovian --upgrade --quiet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elbdVbgdJn1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jovian\n",
        "from nbformat import v4, write"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOhb3WNYZH7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def create_empty_notebook(filename=\"empty.ipynb\"):\n",
        "#   with open(filename, 'w', encoding='utf-8') as f:\n",
        "#     write(v4.new_notebook(), f, version=4)\n",
        "#   return filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ0MGd87ZAQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_notebook(filename=\"colab_session.ipynb\"):\n",
        "  cells = []\n",
        "  for session, line, (input, output) in list(get_ipython().history_manager.get_range(output=True)):\n",
        "    outputs = [v4.new_output(output_type=\"stream\", name=\"stdout\", text=output)] if output else []\n",
        "    cells.append(v4.new_code_cell(execution_count=line, source=input, outputs=outputs))\n",
        "\n",
        "  with open(filename, 'w', encoding='utf-8') as f:\n",
        "    write(v4.new_notebook(cells=cells), f, version=4)\n",
        "  return filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIWmCakjJn2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jovian.commit(project=\"darshansramesh/01-tensor-operations\",filename=create_notebook(), environment=None, is_cli=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qArKxu7Jn2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}